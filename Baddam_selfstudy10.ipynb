{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KoYIcv5rG45-"
   },
   "source": [
    "# Chapter 11\n",
    "# Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AMgLuJaRD8ht"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WUh01n8fEOK-"
   },
   "outputs": [],
   "source": [
    "def split_data(data, prob):\n",
    "    \"\"\"split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    results = [], []\n",
    "    for row in data:\n",
    "        results[0 if random.random() < prob else 1].append(row)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2XV6bw53EdW2"
   },
   "outputs": [],
   "source": [
    "def train_test_split(x, y, test_pct):\n",
    "    data = list(zip(x, y))                        # pair corresponding values\n",
    "    train, test = split_data(data, 1 - test_pct)  # split the dataset of pairs\n",
    "    x_train, y_train = list(zip(*train))          # magical un-zip trick\n",
    "    x_test, y_test = list(zip(*test))\n",
    "    return x_train, x_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hdloonzqFLqo",
    "outputId": "0aa50e32-94db-4204-c561-3bca187f9e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98114\n"
     ]
    }
   ],
   "source": [
    "def accuracy(tp, fp, fn, tn):\n",
    "    correct = tp + tn\n",
    "    total = tp + fp + fn + tn\n",
    "    return correct / total\n",
    "print (accuracy(70, 4930, 13930, 981070))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TILZC1hdF4-q",
    "outputId": "62340153-3c70-4448-df54-2e8d998a0ba8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014\n"
     ]
    }
   ],
   "source": [
    "def precision(tp, fp, fn, tn):\n",
    "    return tp / (tp + fp)\n",
    "print (precision(70, 4930, 13930, 981070))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "60j0TiLvGC0a",
    "outputId": "4894915c-73f2-48ec-e9e5-de203f43418f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005\n"
     ]
    }
   ],
   "source": [
    "def recall(tp, fp, fn, tn):\n",
    "  return tp / (tp + fn)\n",
    "print (recall(70, 4930, 13930, 981070))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K4uMZA6zGLK7"
   },
   "outputs": [],
   "source": [
    "def f1_score(tp, fp, fn, tn):\n",
    "  p = precision(tp, fp, fn, tn)\n",
    "  r = recall(tp, fp, fn, tn)\n",
    "  return 2 * p * r / (p + r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "K2TzKjL1GZLD",
    "outputId": "1598a8f3-8569-45ba-cd82-8e084ae2162b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score(70, 4930, 13930, 981070) 0.00736842105263158\n"
     ]
    }
   ],
   "source": [
    "print(\"f1_score(70, 4930, 13930, 981070)\", f1_score(70, 4930, 13930, 981070))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xmAXxEiMGfzk"
   },
   "source": [
    "# Chapter 13\n",
    "# Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sWb-3a3gG1Dl"
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import math, random, re, glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ascxPLFEHHkW"
   },
   "outputs": [],
   "source": [
    "def tokenize(message):\n",
    "    message = message.lower()                       # convert to lowercase\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)   # extract the words\n",
    "    return set(all_words)                           # remove duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lzYb19yZHPwe"
   },
   "outputs": [],
   "source": [
    "def count_words(training_set):\n",
    "    \"\"\"training set consists of pairs (message, is_spam)\"\"\"\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for message, is_spam in training_set:\n",
    "        for word in tokenize(message):\n",
    "            counts[word][0 if is_spam else 1] += 1\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hPiFO4BxHTnO"
   },
   "outputs": [],
   "source": [
    "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "    \"\"\"turn the word_counts into a list of triplets\n",
    "    w, p(w | spam) and p(w | ~spam)\"\"\"\n",
    "    return [(w,\n",
    "             (spam + k) / (total_spams + 2 * k),\n",
    "             (non_spam + k) / (total_non_spams + 2 * k))\n",
    "             for w, (spam, non_spam) in counts.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pMEG1oUmHWSu"
   },
   "outputs": [],
   "source": [
    "def spam_probability(word_probs, message):\n",
    "    message_words = tokenize(message)\n",
    "    log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
    "\n",
    "    for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
    "\n",
    "        # for each word in the message,\n",
    "        # add the log probability of seeing it\n",
    "        if word in message_words:\n",
    "            log_prob_if_spam += math.log(prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
    "\n",
    "        # for each word that's not in the message\n",
    "        # add the log probability of _not_ seeing it\n",
    "        else:\n",
    "            log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
    "\n",
    "    prob_if_spam = math.exp(log_prob_if_spam)\n",
    "    prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tosr3HWLHaoX"
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "\n",
    "    def train(self, training_set):\n",
    "\n",
    "        # count spam and non-spam messages\n",
    "        num_spams = len([is_spam\n",
    "                         for message, is_spam in training_set\n",
    "                         if is_spam])\n",
    "        num_non_spams = len(training_set) - num_spams\n",
    "\n",
    "        # run training data through our \"pipeline\"\n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts,\n",
    "                                             num_spams,\n",
    "                                             num_non_spams,\n",
    "                                             self.k)\n",
    "\n",
    "    def classify(self, message):\n",
    "        return spam_probability(self.word_probs, message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fS-VUUC7HfDn"
   },
   "outputs": [],
   "source": [
    "def get_subject_data(path):\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # regex for stripping out the leading \"Subject:\" and any spaces after it\n",
    "    subject_regex = re.compile(r\"^Subject:\\s+\")\n",
    "\n",
    "    # glob.glob returns every filename that matches the wildcarded path\n",
    "    for fn in glob.glob(path):\n",
    "        is_spam = \"ham\" not in fn\n",
    "\n",
    "        with open(fn,'r',encoding='ISO-8859-1') as file:\n",
    "            for line in file:\n",
    "                if line.startswith(\"Subject:\"):\n",
    "                    subject = subject_regex.sub(\"\", line).strip()\n",
    "                    data.append((subject, is_spam))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEOXGeGsIODw"
   },
   "outputs": [],
   "source": [
    "def p_spam_given_word(word_prob):\n",
    "    word, prob_if_spam, prob_if_not_spam = word_prob\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fmJKC-iPIQ0Y"
   },
   "outputs": [],
   "source": [
    "def train_and_test_model(path):\n",
    "\n",
    "    data = get_subject_data(path)\n",
    "    random.seed(0)      # just so you get the same answers as me\n",
    "    train_data, test_data = split_data(data, 0.75)\n",
    "\n",
    "    classifier = NaiveBayesClassifier()\n",
    "    classifier.train(train_data)\n",
    "\n",
    "    classified = [(subject, is_spam, classifier.classify(subject))\n",
    "              for subject, is_spam in test_data]\n",
    "\n",
    "    counts = Counter((is_spam, spam_probability > 0.5) # (actual, predicted)\n",
    "                     for _, is_spam, spam_probability in classified)\n",
    "\n",
    "    print(counts)\n",
    "\n",
    "    classified.sort(key=lambda row: row[2])\n",
    "    spammiest_hams = list(filter(lambda row: not row[1], classified))[-5:]\n",
    "    hammiest_spams = list(filter(lambda row: row[1], classified))[:5]\n",
    "\n",
    "    print(\"spammiest_hams\", spammiest_hams)\n",
    "    print(\"hammiest_spams\", hammiest_spams)\n",
    "\n",
    "    words = sorted(classifier.word_probs, key=p_spam_given_word)\n",
    "\n",
    "    spammiest_words = words[-5:]\n",
    "    hammiest_words = words[:5]\n",
    "\n",
    "    print(\"spammiest_words\", spammiest_words)\n",
    "    print(\"hammiest_words\", hammiest_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "LTmB0aInIhj4",
    "outputId": "2e574032-ad19-4ecf-e725-be9f18c845be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n",
      "spammiest_hams []\n",
      "hammiest_spams []\n",
      "spammiest_words []\n",
      "hammiest_words []\n"
     ]
    }
   ],
   "source": [
    "train_and_test_model(r\"/home/joel/src/spam/*/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rk1vxzFWIwmD"
   },
   "source": [
    "# Chapter 17\n",
    "# Decision Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xc2TmaNxI2GJ"
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from functools import partial\n",
    "import math, random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ipOkp0NJkRp"
   },
   "outputs": [],
   "source": [
    "def entropy(class_probabilities):\n",
    "    \"\"\"given a list of class probabilities, compute the entropy\"\"\"\n",
    "    return sum(-p * math.log(p, 2) for p in class_probabilities if p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eOs-gxTNJmRZ"
   },
   "outputs": [],
   "source": [
    "def class_probabilities(labels):\n",
    "    total_count = len(labels)\n",
    "    return [count / total_count\n",
    "            for count in Counter(labels).values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8GqwV3HJrmb"
   },
   "outputs": [],
   "source": [
    "def data_entropy(labeled_data):\n",
    "    labels = [label for _, label in labeled_data]\n",
    "    probabilities = class_probabilities(labels)\n",
    "    return entropy(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_MSZCnihJtva"
   },
   "outputs": [],
   "source": [
    "def partition_entropy(subsets):\n",
    "    \"\"\"find the entropy from this partition of data into subsets\"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "\n",
    "    return sum( data_entropy(subset) * len(subset) / total_count\n",
    "                for subset in subsets )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kUNGDeAJ1Na"
   },
   "outputs": [],
   "source": [
    "inputs = [\n",
    "({'level':'Senior', 'lang':'Java', 'tweets':'no', 'phd':'no'}, False),\n",
    "({'level':'Senior', 'lang':'Java', 'tweets':'no', 'phd':'yes'}, False),\n",
    "({'level':'Mid', 'lang':'Python', 'tweets':'no', 'phd':'no'}, True),\n",
    "({'level':'Junior', 'lang':'Python', 'tweets':'no', 'phd':'no'}, True),\n",
    "({'level':'Junior', 'lang':'R', 'tweets':'yes', 'phd':'no'}, True),\n",
    "({'level':'Junior', 'lang':'R', 'tweets':'yes', 'phd':'yes'}, False),\n",
    "({'level':'Mid', 'lang':'R', 'tweets':'yes', 'phd':'yes'}, True),\n",
    "({'level':'Senior', 'lang':'Python', 'tweets':'no', 'phd':'no'}, False),\n",
    "({'level':'Senior', 'lang':'R', 'tweets':'yes', 'phd':'no'}, True),\n",
    "({'level':'Junior', 'lang':'Python', 'tweets':'yes', 'phd':'no'}, True),\n",
    "({'level':'Senior', 'lang':'Python', 'tweets':'yes', 'phd':'yes'}, True),\n",
    "({'level':'Mid', 'lang':'Python', 'tweets':'no', 'phd':'yes'}, True),\n",
    "({'level':'Mid', 'lang':'Java', 'tweets':'yes', 'phd':'no'}, True),\n",
    "({'level':'Junior', 'lang':'Python', 'tweets':'no', 'phd':'yes'}, False)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mb35UO1IJ8dU"
   },
   "outputs": [],
   "source": [
    "def group_by(items, key_fn):\n",
    "    \"\"\"returns a defaultdict(list), where each input item\n",
    "    is in the list whose key is key_fn(item)\"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for item in items:\n",
    "        key = key_fn(item)\n",
    "        groups[key].append(item)\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kiZwMNuzKB71"
   },
   "outputs": [],
   "source": [
    "def partition_by(inputs, attribute):\n",
    "    \"\"\"returns a dict of inputs partitioned by the attribute\n",
    "    each input is a pair (attribute_dict, label)\"\"\"\n",
    "    return group_by(inputs, lambda x: x[0][attribute])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kMrG-V_AKEJ8"
   },
   "outputs": [],
   "source": [
    "def partition_entropy_by(inputs,attribute):\n",
    "    \"\"\"computes the entropy corresponding to the given partition\"\"\"\n",
    "    partitions = partition_by(inputs, attribute)\n",
    "    return partition_entropy(partitions.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "_ahyWEWkKIDt",
    "outputId": "d2378c63-5039-43e4-929a-905b937ff5e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.6935361388961919\n",
      "lang 0.8601317128547441\n",
      "tweets 0.7884504573082896\n",
      "phd 0.8921589282623617\n"
     ]
    }
   ],
   "source": [
    "for key in ['level','lang','tweets','phd']:\n",
    "  print (key, partition_entropy_by(inputs, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eCQRLuYvKLJA"
   },
   "outputs": [],
   "source": [
    "senior_inputs = [(input, label)\n",
    "                 for input, label in inputs if input[\"level\"] == \"Senior\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "n60Y87lMKSc4",
    "outputId": "df62f5d8-11dc-4c7b-b982-2600972766ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang 0.4\n",
      "tweets 0.0\n",
      "phd 0.9509775004326938\n"
     ]
    }
   ],
   "source": [
    "for key in ['lang', 'tweets', 'phd']:\n",
    "  print (key, partition_entropy_by(senior_inputs, key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYKji7vyKXiz"
   },
   "outputs": [],
   "source": [
    "def classify(tree, input):\n",
    "    \"\"\"classify the input using the given decision tree\"\"\"\n",
    "\n",
    "    # if this is a leaf node, return its value\n",
    "    if tree in [True, False]:\n",
    "        return tree\n",
    "\n",
    "    # otherwise find the correct subtree\n",
    "    attribute, subtree_dict = tree\n",
    "\n",
    "    subtree_key = input.get(attribute)  # None if input is missing attribute\n",
    "\n",
    "    if subtree_key not in subtree_dict: # if no subtree for key,\n",
    "        subtree_key = None              # we'll use the None subtree\n",
    "\n",
    "    subtree = subtree_dict[subtree_key] # choose the appropriate subtree\n",
    "    return classify(subtree, input)     # and use it to classify the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PHCSpmOYKeP6"
   },
   "outputs": [],
   "source": [
    "def build_tree_id3(inputs, split_candidates=None):\n",
    "\n",
    "    # if this is our first pass,\n",
    "    # all keys of the first input are split candidates\n",
    "    if split_candidates is None:\n",
    "        split_candidates = inputs[0][0].keys()\n",
    "\n",
    "    # count Trues and Falses in the inputs\n",
    "    num_inputs = len(inputs)\n",
    "    num_trues = len([label for item, label in inputs if label])\n",
    "    num_falses = num_inputs - num_trues\n",
    "\n",
    "    if num_trues == 0:                  # if only Falses are left\n",
    "        return False                    # return a \"False\" leaf\n",
    "\n",
    "    if num_falses == 0:                 # if only Trues are left\n",
    "        return True                     # return a \"True\" leaf\n",
    "\n",
    "    if not split_candidates:            # if no split candidates left\n",
    "        return num_trues >= num_falses  # return the majority leaf\n",
    "\n",
    "    # otherwise, split on the best attribute\n",
    "    best_attribute = min(split_candidates,\n",
    "        key=partial(partition_entropy_by, inputs))\n",
    "\n",
    "    partitions = partition_by(inputs, best_attribute)\n",
    "    new_candidates = [a for a in split_candidates\n",
    "                      if a != best_attribute]\n",
    "\n",
    "    # recursively build the subtrees\n",
    "    subtrees = { attribute : build_tree_id3(subset, new_candidates)\n",
    "                 for attribute, subset in partitions.items() }\n",
    "\n",
    "    subtrees[None] = num_trues > num_falses # default case\n",
    "\n",
    "    return (best_attribute, subtrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e-iNrHAiKlui"
   },
   "outputs": [],
   "source": [
    "def forest_classify(trees, input):\n",
    "    votes = [classify(tree, input) for tree in trees]\n",
    "    vote_counts = Counter(votes)\n",
    "    return vote_counts.most_common(1)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vFToZHGFKoN2"
   },
   "outputs": [],
   "source": [
    "tree = build_tree_id3(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9u9dSdlLKzdm",
    "outputId": "e41fdb3d-46b7-4ec3-904c-19bc636c6830"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(tree, { \"level\" : \"Junior\",\n",
    "                \"lang\" : \"Java\",\n",
    "                \"tweets\" : \"yes\",\n",
    "                \"phd\" : \"no\"} ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wLTREXfqLUwy",
    "outputId": "b92cf660-ee0c-4f87-d4bd-737c8714ec47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(tree, { \"level\" : \"Junior\",\n",
    "                \"lang\" : \"Java\",\n",
    "                \"tweets\" : \"yes\",\n",
    "                \"phd\" : \"yes\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4ddgvbr1LY2S",
    "outputId": "e2613220-045c-4f19-ecd3-a4848b61b875"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "classify(tree, { \"level\" : \"Senior\" } ) # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9vPFlzu_LbAT",
    "outputId": "159a73f0-717d-4ec9-df0f-2e5ca9c9284f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(tree, { \"level\" : \"Intern\" } ) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AQ12E4M7Ld5i"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Teegala_selfstudy10.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
